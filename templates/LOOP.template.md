# Iteration Loop

<!--
TEMPLATE INSTRUCTIONS:
1. Customize steps for your domain
2. Define your preparation, execution, and capture phases
3. Set up review cycle frequencies
4. Remove this instruction block when done
-->

The core cycle: Prepare → Execute → Evaluate → Capture → Improve → Repeat

---

## Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                      THE ITERATION LOOP                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│     ┌──────────┐                                                │
│     │ PREPARE  │  Load context, consult memory                  │
│     └────┬─────┘                                                │
│          │                                                      │
│          ▼                                                      │
│     ┌──────────┐                                                │
│     │ EXECUTE  │  Do the work, observe as you go                │
│     └────┬─────┘                                                │
│          │                                                      │
│          ▼                                                      │
│     ┌──────────┐                                                │
│     │ EVALUATE │  Rate output, assess quality                   │
│     └────┬─────┘                                                │
│          │                                                      │
│          ▼                                                      │
│     ┌──────────┐                                                │
│     │ CAPTURE  │  Document learnings, log metrics               │
│     └────┬─────┘                                                │
│          │                                                      │
│          ▼                                                      │
│     ┌──────────┐                                                │
│     │ IMPROVE  │  Update process, prepare next                  │
│     └────┬─────┘                                                │
│          │                                                      │
│          └──────────────────────┐                               │
│                                 ▼                               │
│                            [REPEAT]                             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Step 1: Prepare

Before starting the iteration:

### Consult Memory
```
□ Read LEARNING.md for relevant knowledge
□ Check METRICS.md for recent patterns
□ Review recent OUTPUTS.md for context
```

### Set Intent
```
□ What is the goal of this iteration?
□ What am I exploring or producing?
□ What hypotheses am I testing?
```

### Check Constraints
```
□ Time available: [Duration]
□ Resources needed: [List]
□ Dependencies: [Any blockers?]
```

---

## Step 2: Execute

During the iteration:

### Do the Work
[Customize for your domain]:
- [Primary activity 1]
- [Primary activity 2]
- [Primary activity 3]

### Observe
As you work, notice:
- What's working well?
- What's difficult?
- What's surprising?
- What questions arise?

### Capture in Real-Time
Keep running notes:
```
□ Observations made
□ Decisions recorded
□ Surprises noted
□ Questions captured
```

---

## Step 3: Evaluate

After completing:

### Rate the Output

Using the quality scale from METRICS.md:

| Rating | Criteria Met |
|--------|--------------|
| 5/5 | [All criteria] |
| 4/5 | [Most criteria] |
| 3/5 | [Some criteria] |
| 2/5 | [Few criteria] |
| 1/5 | [Failed] |

**This iteration's rating**: [X/5]

### Assess Against Prior Work
- Better than last similar output?
- Trend direction?
- New high/low water mark?

### Identify Root Causes
For successes:
- What specifically worked?
- Can it be replicated?

For failures:
- What specifically failed?
- How to prevent next time?

---

## Step 4: Capture

Document everything:

### Update LEARNING.md
```markdown
### [Category/Technique]
- **[Discovery]**: [What was learned]
  - Observed: [This iteration]
  - Confidence: [Level]
```

### Log in METRICS.md
```markdown
| [Date] | [N] outputs | [X/5] rating | [Category] | [Notes] |
```

### Rate in OUTPUTS.md
```markdown
## [Rating Tier]
- [Output name/ID]: [Brief description]
```

### Log Issues in ISSUES.md
```markdown
| [ID] | [Category] | [Issue] | [Severity] | [Status] |
```

---

## Step 5: Improve

Based on this iteration:

### Process Adjustments
- What would make next iteration better?
- Any process steps to add/remove/modify?

### Knowledge Connections
- How does this connect to existing knowledge?
- Any patterns strengthening?

### Next Iteration Setup
- What to explore next?
- Any hypotheses to test?

---

## Review Cycles

### Micro: Every Iteration
- Complete Steps 1-5 above
- Duration: [Typical time]

### Meso: Every [N] Iterations

```markdown
## Retrospective: Iterations [X] to [Y]

### What Worked
- [Successes to continue]

### What Didn't Work
- [Issues to address]

### Patterns Emerging
- [Observations ready for promotion]

### Process Changes
- [Modifications to the loop]

### Focus for Next [N]
- [Priorities]
```

### Macro: Every [M] Iterations

```markdown
## Strategic Review: Iteration [N]

### Overall Performance
- Quality trend: [Improving/Stable/Declining]
- Learning velocity: [High/Medium/Low]
- System health: [Good/Concerns/Critical]

### Knowledge Consolidation
- Principles confirmed: [List]
- Anti-patterns established: [List]
- Areas of uncertainty: [List]

### Strategic Direction
- Continue: [What to keep doing]
- Stop: [What to cease]
- Start: [What to begin]

### System Evolution
- Process improvements: [Changes to loop]
- Documentation updates: [Changes to docs]
- Architecture changes: [Structural changes]
```

---

## Anti-Patterns

Avoid these behaviors:

| Anti-Pattern | Problem | Instead |
|--------------|---------|---------|
| Skip preparation | Repeating mistakes | Always consult memory |
| No real-time capture | Lost observations | Note as you go |
| Inflated ratings | No signal | Rate honestly |
| Skip documentation | Lost knowledge | Always capture |
| Same approach always | Local optima | Explore regularly |
| Skip retrospectives | No meta-improvement | Honor the schedule |

---

## Exploration Mandate

Each iteration should include some exploration:

- Try something you haven't tried in [N]+ iterations
- Test a hypothesis from LEARNING.md
- Push a parameter to an extreme
- Deliberately break a "rule" to see what happens

Exploitation wins the short term. Exploration wins the long term.

---

## Documentation Quick Reference

| When | Document | Action |
|------|----------|--------|
| Before iteration | LEARNING.md | Consult |
| Before iteration | METRICS.md | Check trends |
| During iteration | Notes | Capture observations |
| After iteration | LEARNING.md | Add discoveries |
| After iteration | METRICS.md | Log metrics |
| After iteration | OUTPUTS.md | Rate outputs |
| Every [N] iterations | All docs | Retrospective |

---

## The Loop Imperative

**Every iteration should leave the system smarter than before.**

The loop is not overhead—it is the mechanism of improvement.
