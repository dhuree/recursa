# Metrics & Evaluation

<!--
TEMPLATE INSTRUCTIONS:
1. Define your iteration metrics
2. Customize quality rating scale
3. Set up tracking templates
4. Remove this instruction block when done
-->

Quantitative tracking enables data-driven improvement.

---

## Philosophy

**"What gets measured gets improved."**

But also: **"Not everything measured matters, and not everything that matters can be measured."**

Balance numbers with judgment.

---

## Core Metrics

### Iteration Metrics

Track every iteration:

| Metric | Definition | Target |
|--------|------------|--------|
| **[Outputs created]** | Number of outputs per iteration | [X] |
| **[Quality rate]** | Outputs rated [X]+/5 / Total | [Y]% |
| **[Success rate]** | Outputs meeting criteria / Total | [Z]% |
| **[Discoveries]** | New learnings captured | [N]+ |
| **[Time spent]** | Duration of iteration | [Benchmark] |

### Category Metrics

Track by [category/technique/type]:

| Metric | Definition | Use |
|--------|------------|-----|
| **Exploration count** | Total outputs in category | Coverage |
| **Success rate** | High-quality / Total | Productivity |
| **Average rating** | Mean quality score | Fine-grained quality |
| **Last explored** | Iterations since last use | Prevent neglect |

### Process Metrics

Track the system itself:

| Metric | Definition | Signals |
|--------|------------|---------|
| **Learning velocity** | Discoveries per iteration | Knowledge growth |
| **Pattern promotions** | Observations → Patterns | System maturity |
| **Issue resolution** | Fixed / Found | Technical health |
| **Exploration diversity** | Spread across categories | Rut detection |

---

## Quality Rating Scale

| Rating | Name | Criteria |
|--------|------|----------|
| **5/5** | Excellent | [Definition for your domain] |
| **4/5** | Strong | [Definition for your domain] |
| **3/5** | Adequate | [Definition for your domain] |
| **2/5** | Weak | [Definition for your domain] |
| **1/5** | Failed | [Definition for your domain] |

### Rating Guidance

**5/5 requires**:
- [Criterion 1]
- [Criterion 2]
- [Criterion 3]

**Common deductions**:
- [Issue that drops 1 point]
- [Issue that drops 2 points]

---

## Self-Assessment Protocol

### After Each Iteration

```markdown
## Iteration [N] Metrics

### Outputs
- Created: [N]
- Rated: [N]
  - 5/5: [N]
  - 4/5: [N]
  - 3/5: [N]
  - <3/5: [N]

### Process
- Time spent: [Duration]
- Blockers: [Any/None]

### Learning
- Discoveries: [N]
- Confirmations: [N]

### Self-Critique
- What worked: [Description]
- What didn't: [Description]
- Next iteration: [Focus]
```

### Every [N] Iterations (Retrospective)

```markdown
## Retrospective: Iterations [X] to [Y]

### Quantitative Summary
- Total outputs: [N]
- High-quality outputs: [N] ([X]%)
- Best result: [Description]
- Worst result: [Description]

### Category Performance
| Category | Outputs | 4+/5 | Rate | Trend |
|----------|---------|------|------|-------|
| [Cat 1]  | [N]     | [N]  | [X]% | ↑/→/↓ |

### Learning
- Discoveries: [N]
- Patterns promoted: [N]
- Anti-patterns added: [N]

### Process Health
- Loop followed: [Yes/No/Partial]
- Documentation current: [Yes/No]
- Issues accumulating: [Yes/No]

### Action Items
- [ ] [What to change]
- [ ] [What to improve]
```

---

## Tracking Templates

### Iteration Log

```markdown
| Iter | Date | Outputs | 5/5 | 4+/5 | Categories | Notes |
|------|------|---------|-----|------|------------|-------|
| 1 | [Date] | [N] | [N] | [N] | [List] | [Brief] |
```

### Category Dashboard

```markdown
| Category | Total | 5/5 | 4+/5 | Rate | Last | Status |
|----------|-------|-----|------|------|------|--------|
| [Cat 1] | [N] | [N] | [N] | [X]% | Iter [N] | [Status] |
```

### Discovery Tracking

```markdown
| Period | Discoveries | Promoted | Tested | Velocity |
|--------|-------------|----------|--------|----------|
| Iter 1-5 | [N] | [N] | [N] | [High/Med/Low] |
```

---

## Analysis Methods

### Trend Analysis

Look for patterns over time:
- Is quality improving?
- Are plateaus forming?
- What breaks through plateaus?

### Pareto Analysis

Find the vital few:
- Which categories produce most successes?
- Which produce most failures?
- Where to focus effort?

### Correlation Analysis

What predicts success?
- Which approaches correlate with high ratings?
- Which correlate with failures?

---

## Metric-Driven Decisions

Use metrics to trigger actions:

| Condition | Action |
|-----------|--------|
| Category unused [N]+ iterations | Include in next iteration |
| Category success rate < [X]% | Investigate or deprioritize |
| No discoveries in [N] iterations | Force novelty |
| Quality plateau [N]+ iterations | Try new approach |
| Issue count growing | Schedule fix session |

---

## Avoiding Pitfalls

### Goodhart's Law
"When a measure becomes a target, it ceases to be a good measure."

Don't game metrics. Measure outcomes, not activities.

### Vanity Metrics
Avoid metrics that feel good but don't drive improvement.

### Over-Measurement
Track only what drives decisions. If a metric never influences action, drop it.

---

## The Metrics Imperative

**Track consistently. Analyze honestly. Act decisively.**

Metrics transform gut feelings into knowledge.
